# -------------------------------------------------------------------
#  CATSERL – minimal config for DQN + buffers
# -------------------------------------------------------------------

seed: 42                 # global RNG seed
device: "cpu"           # "cpu" or "cuda"

# -------------------------------------------------
#  Dueling-DQN worker  (rl/dqn.py)
# -------------------------------------------------
dqn:
  hidden_dim:        128        # width of shared + stream layers
  lr:                5e-4       # Adam LR (same for all params)
  gamma:             0.95       # discount
  tau:               0.005      # soft-update factor for target net

  # replay + SGD
  buffer_size:       50_000    # main ReplayBuffer capacity
  batch_size:        32

  # ε-greedy exploration schedule
  eps_start:         1.0
  eps_end:           0.05
  eps_decay_frames:  3_000

# -------------------------------------------------
# PDERL parameters   (pderl/)
pderl:
  pop_size:          0         # subpopulation size of each pderl island
  migrate_every_gens:  15     # copy RL actor into EA population every X generations
  bc_epochs:    1
  bc_batch:     256
  crossover_lr: 1e-3

# -------------------------------------------------
# MOPDERL parameters   (pderl/)
mopderl:
  episodes_per_actor: 5   # (example value, set as appropriate)


# -------------------------------------------------
#  Buffers for genetic actors (data/buffers.py)
# -------------------------------------------------
mini_buffer_size: 8192          # per-genome circular buffer length

# -------------------------------------------------
#  Environment wrapper (envs/four_rooms.py)
#  — only the bits needed to compute rewards here —
# -------------------------------------------------
env:
  name: "four-room-v0"
  num_objectives: 3             # m  (set by your research task)
  beta_novelty: 0.05             # intrinsic-reward scale
  max_ep_len: -1                # max episode length, -1 for no limit