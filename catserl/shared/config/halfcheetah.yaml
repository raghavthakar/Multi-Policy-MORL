# -------------------------------------------------------------------
#  CATSERL
# -------------------------------------------------------------------

seed: 2024                 # Global RNG seed
device: "cpu"            # "cpu" or "cuda"

pderl:
  pop_size: 10

# -------------------------------------------------
#  RL (Island Stage)
# -------------------------------------------------
rl:
  hidden_dim:        256
  gamma:             0.99
  tau:               0.005
  update_every_n_steps: 1
  updates_per_session: 1
  total_timesteps: 4_000_000
  start_timesteps: 25_000

  td3:
    critic_lr:       3e-4
    actor_lr:        3e-4
    discount:        0.99
    policy_noise:    0.2
    noise_clip:      0.5
    policy_freq:     2
    exploration_noise: 0.1
    buffer_size:     1_000_000
    batch_size:      256

# -------------------------------------------------
#  MO-PDERL (Multi-Objective Stage)
# -------------------------------------------------
mopderl:
  # Evaluation settings for actors in the population
  episodes_per_actor: 10

  # Settings for creating the child's static offline dataset
  child_buffer_size: 200_000

  # Hyperparameters for the AWR fine-tuning process
  finetune:
    epochs:          50
    lr:              3e-4
    batch_size:      256
    awr_beta:        1.0      # Temperature for continuous AWR
    awr_clip:        20.0     # Max value for AWR weights
    pretrain_steps:  20000    # how many behaviour cloning steps

# -------------------------------------------------
#  Buffers for Genetic Actors
# -------------------------------------------------
mini_buffer_size: 50_000

# -------------------------------------------------
#  Environment wrapper (envs/four_rooms.py)
#  — only the bits needed to compute rewards here —
# -------------------------------------------------
env:
  name: 'mo-halfcheetah-v5'
  num_objectives: 2            # m  (depends on env)
  max_ep_len: -1               # max episode length, -1 for no limit
